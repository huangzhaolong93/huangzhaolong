<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[markdown语法]]></title>
    <url>%2F2018%2F11%2F07%2Fmarkdown%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[标题#### 你好 你好列表* 你好 你好 - 你好 你好 #加粗样例: **你好** 你好 __你好__ 你好 #斜体*你好* 你好 _你好_ _你好_ 字体的类型/大小/颜色\我是黑体\ 我是黑体 \我是华文彩云\ 我是华文彩云 \我是1号字体\ 我是1号字体 \我是3号字体\ 我是3号字体 \我色号是#0099ff\ 我色号是#0099ff \我色号是#FF0000\ 我色号是#FF0000f 公式数学公式 \$行内公式\$: $a+b$ \$ \$行间公式\$\$: $$a+b$$ 表格 a b 1 2 链接[链接名字](链接url) 百度]]></content>
      <categories>
        <category>工具</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[逻辑斯蒂回归-LR]]></title>
    <url>%2F2018%2F11%2F07%2F%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[理论基础&emsp;&emsp;逻辑斯蒂回归是机器学习里的一种分类模型。本节将介绍逻辑斯蒂回归相关的理论基础知识。 相关知识&emsp;&emsp;最大似然的思想：假如有一个罐子，罐子里有黑白两种颜色的球，球的数目不知，球的颜色比例也不知。这时候我们从罐子中有放回的拿出10个球，即拿出1个球，记录颜色，再放回罐中摇匀，此操作重复10次。假设记录结果显示拿出了7个白球，3个黑球。请问罐中黑白球比例最可能是多少？大部分人都会毫不犹豫的给出答案，$黑:白=3:7$。这背后运用到理论支持其实就是最大似然的思想。&emsp;&emsp;似然，指的就是使这种情况发生。最大似然就是使这种情况最大概率发生。接下来，我们来看看最大似然思想是如何得到这样的结果。&emsp;&emsp;假设抽到黑球的概率是$P$,则抽到白球的概率就是$1-P$。此时抽了10次，抽到了7个白球3个黑球，这种情况的概率就是$P(3黑7白)=P^3(1-P)^7$。为了是这种情况最大概率发生，我们可以得到$$arg\,\max_{P} P^3(1-P)^7$$令导数为0，得到公式$$3P^2(1-P)^7+(-7)P^3(1-P)^6=0$$解得P=0.3.故抽到黑球的比例是0.3，黑球和白球的比例就是$3:7$。 模型特点 逻辑斯蒂回归的模型能力：分类&emsp;给定样本x的特征$ X=\lbrace x_1,x_2,…,x_n\rbrace $,判断样本的标签$Y$。 逻辑斯蒂回归的学习方法：有监督学习&emsp;需要有训练样本对模型进行训练。 逻辑斯蒂回归的模型类型：判别式模型&emsp;通过训练的特征$X$和类别$Y$，直接计算$P(Y|X)$作为模型，直接根据样本的特征$X$计算类别$Y$。 模型推导&emsp;&emsp;最大似然的思想可以总结为”透过现象看本质”。根据上面黑白球的例子，现象就是抽了10个球，3黑7白，本质就是抽到黑球的概率是0.3。而类比在逻辑斯蒂回归中，现象就是训练样本的特征和标签，本质就是逻辑斯蒂回归(LR)模型的模型参数。我们的任务就是寻找一组LR模型的参数，使得训练样本的特征和标签出现的概率最大。&emsp;&emsp;假设我们有如下m个样本，每个样本有n维特征：$$\lbrace x_1^{(1)},x_2^{(1)},…,x_n^{(1)},y^{(1)}\rbrace,\lbrace x_1^{(2)},x_2^{(2)},…,x_n^{(2)},y^{(2)}\rbrace,…,\lbrace x_1^{(m)},x_2^{(m)},…,x_n^{(m)},y^{(m)}\rbrace$$&emsp;&emsp;标签$y\in{0,1}$。LR模型的模型参数为$\theta$,根据最大似然的思想，我们认为这m个样本是已经发生的现实情况，故将问题转化为$$arg\,\max_{\theta}\prod_{x_i\in X}{P(Y=0|x_i,\theta)}^{(1-y_i)}{P(Y=1|x_i,\theta)}^{y_i}$$&emsp;&emsp;其中，$P(Y|X,\theta)$就是LR模型。&emsp;&emsp;我们可以看到，逻辑斯蒂回归模型其实是一个分类模型，并不是回归模型。那为什么会有这么一个名字呢？笔者认为逻辑斯蒂回归可以拆解为逻辑斯蒂(logistic)+线性回归。线性回归模型是一个回归模型，模型可以表示为$$y=\theta_1x_1+\theta_2x_2+…+\theta_nx_n=\mathrm{\theta}^\mathrm{T}\mathrm{x}$$&emsp;&emsp;模型的取值为$-\infty\sim+\infty$。那如何转化为分类问题呢，就需要一个函数映射，将$-\infty\sim+\infty$映射成$0\sim1$(概率表示)且具有连续、光滑、严格单调和关于中点中心对称。而logistic函数(sigmoid)恰好满足所有的需求。logistic函数为$y=\frac{1}{1+e^{-x}}$。绘制函数图像的代码如下： 12345678910111213141516import matplotlib.pyplot as plt import numpy as np import math&apos;&apos;&apos;def sigmoid(x): return (1/(1+math.exp(-x)))#如果用上面的sigmoid，会导致分母溢出而报错，推荐用下面的sigmoid自定义函数print(sigmoid(-10000000))&apos;&apos;&apos;def sigmoid(x): return math.exp(-np.lnaddexp(0, -x)) x = np.arange(-10., 10., 0.2)y = [ sigmoid(x_i) for x_i in x]plt.plot(x, y) plt.show() &emsp;&emsp;综合logistic函数和线性回归模型函数，我们可以得到逻辑斯蒂回归模型的模型函数$$y=\frac{1}{1+e^{-\mathrm{\theta}^T\mathrm{x}}}$$&emsp;&emsp;我们令$h(\theta,x)$表示LR模型的模型函数，即$h(\theta,x)=\frac{1}{1+e^{-\theta^\mathrm{T}x}}$。如果$h(\theta,x)=0.8$，我们可以认为该样本有$0.8$的概率是正样本，$0.2$的概率是负样本，即$P(y=1|x,\theta) = h(\theta,x)$和$P(y=0|x,\theta) = 1-h(\theta,x)$。将LR的模型函数代回到最大似然的求解问题中，得到$$arg\,\max_{\theta}\prod_{x_i\in X}(1-h(\theta,x))^{(1-y_i)}h(\theta,x)^{y_i}$$&emsp;&emsp;由于是累乘，我们使用单调的算子$ln(\cdot)$将累乘变成累加，且不影响最终结果。得到$$arg\,\max_{\theta}\sum_{x_i\in X}(1-y_i)\ln (1-h(\theta,x))+y_i\ln h(\theta,x)$$&emsp;&emsp;我们令$$L(\theta)=\sum_{x_i\in X}(1-y_i)\ln (1-h(\theta,x))+y_i\ln h(\theta,x)$$&emsp;&emsp;若要求解上述的$argmax$，则需要$L^\prime(\theta)=0$且$L^{\prime\prime}(\theta)&lt;0$。根据LR模型的函数，有$$\begin{eqnarray}L(\theta) &amp;=&amp;\sum_{x_i\in X}(1-y_i)\ln (1-\frac{1}{1+e^{-\mathrm{\theta}^T\mathrm{x}}})+y_i\ln \frac{1}{1+e^{-\mathrm{\theta}^T\mathrm{x}}}\ &amp;=&amp;\sum_{x_i\in X}y_i(\mathrm{\theta}^T\mathrm{x})-\ln(1+e^{\mathrm{\theta}^T\mathrm{x}}) \end{eqnarray}$$&emsp;&emsp;由于$L(\theta)$是多变量的，故采用梯度上升法进行求解其最大值。如果要求$L(\theta)$的最大值，也就是等价于求$-L(\theta)$的最小值，此时$-L(\theta)$可以理解成逻辑斯蒂回归模型的损失函数。最终问题等价于用梯度下降法求解$-L(\theta)$的最小值。梯度下降法的参数更新过程为：$$\theta_j := \theta_j - \alpha \frac{\partial L(\theta)}{\partial \theta_j} ,(j=0,…,n)$$&emsp;&emsp;其中$\alpha$为学习率。那么$L(\theta)$关于$\theta_j$的偏导数为$$\begin{eqnarray}\frac{\partial L(\theta)}{\partial \theta_j} &amp;=&amp;\alpha\left(\sum_{x_i\in X}{y_ix_{ij}-x_{ij}\frac{e^{\mathrm{\theta}^T}}{1+e^{\mathrm{\theta}^T\mathrm{x}}}}\right)\ &amp;=&amp;\alpha\sum_{x_i\in X}x_{ij}(y_i-h(\theta,x_i))\ &amp;=&amp;\alpha\sum_{x_i\in X}x_{ij}\Delta y_i\end{eqnarray}$$&emsp;&emsp;上述的公式表达着这样一个道理：第$j$维特征上的模型参数，它的更新方向为学习率(即学习步长)$\alpha$乘上每个样本$x_i$在第$j$维度特征上真实值$y_j$和模型预测值$h(\theta,x_i)$的差。简单来说，就是参数更新的方向是通过模型误差乘上学习步长进行调整的。 进阶优化特征离散化，将连续的特征离散化成一系列的0和1。&emsp;&emsp;1、特征容易增加或减少，特征解释性强，例如年龄这维特征$X_{age}$，比如第一版模型中，特征是$X_{age&lt;18}$，这维特征可以理解成是否成年。后续第二版是可以直接增加$X_{18&lt;age&lt;25}$的特征，可以理解为是否为青年人。&emsp;&emsp;2、特征$X$变成0或1，$y=sigmoid(\theta^TX)$中$\theta^TX)$将变成稀疏向量，其内积乘法运算速度快，计算结果方便存储，容易扩展。&emsp;&emsp;3、特征离散化后，具有极强的鲁棒性。例如年龄这维特征$X_{age}$，特征是$X_{age&lt;18}$。如果年龄数据中出现录入错误的，比如录入了身高数据175，在$X_{age&lt;18}$这个特征中也仅仅是1，对结果影响没有那么大。但是如果采用的是连续特征，那么$\theta_{age}X_{age}$会因为$X_{age}=175$变得很大。导致结果偏差很大。&emsp;&emsp;4、LR模型本质上还是线性模型，本质上仍然是$\theta^TX$，如果不离散化，年龄特征$X_{age}$只会有一个系数$\theta_{age}$，如果离散化，年龄特征可能会变成$X_{age&lt;18}$,$X_{age&lt;30}$,$X_{30&lt;age&lt;40}$等等一系列离散特征，这样每个特征前的参数都不一样，相当于对于年龄这位特征是分段线性的，提升模型的表达能力。&emsp;&emsp;5、特征离散化后，有利于进行特征交叉。如果不做离散化，可能就只有$X_{age}$和$X_{sex}$的交叉，离散化之后，就可以是$X_{age&lt;18}X_{sex=F}$表示女性未成年，$X_{18&lt;age&lt;25}X_{sex=M}$表示男性青年。特征A离散化为$M$个值，特征B离散为$N$个值，那么交叉之后会有$M\times N$个变量，进一步引入非线性，提升表达能力；&emsp;&emsp;6、特征离散化后，模型更加稳定。比如特征重要度最高的是年龄特征。离散化后为$X_{30&lt;age&lt;40}$，那么年龄在$30-40$岁值都是一样的，不会因为年龄大了一岁，就使得最后的结果发生较大变化。&emsp;&emsp;7、特征离散化后，通过将特征分散，起到弱化特征的作用，避免某一维度特征过大，从而导致过拟合。比如年龄特征$X_{age}$的特征重要度$\theta_{age}$很大，那么模型会过分依赖这维特征。一旦$X_{age}$出现了微小的波动，则都会对模型结果产生巨大的影响。 工程实现代码中LR_classify为LR分类器模型，具体代码如下。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107#!/usr/bin/env python3# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Thu Aug 23 19:52:02 2018@author: huangzhaolong&quot;&quot;&quot;from sklearn.datasets import load_irisimport numpy as npimport randomimport mathdef load_data(): iris = load_iris() X = iris.data Y = iris.target #特征离散化,多分类转二分类 X = X[Y&lt;=1].round() Y = Y[Y&lt;=1] data_num = X.shape[0] index_list = [random.random() &gt;= 0.2 for _ in range(data_num)] train_index_list = [] test_index_list = [] for i in range(len(index_list)): if index_list[i]: train_index_list.append(i) else: test_index_list.append(i) X_train = X[train_index_list] X_test = X[test_index_list] Y_train = Y[train_index_list] Y_test = Y[test_index_list] return X_train, X_test, Y_train, Y_testclass LR_classify(object): def sigmoid(self, num_list): if isinstance(num_list, (int, float)): return 1/(1+math.exp(-num_list)) else: result_list = [] for num in num_list: if isinstance(num, (int, float)): result_list.append(1/(1+math.exp(-num))) else: print (&quot;error type!&quot;) return -1 return result_list def LR_train(self, X_train, Y_train, train_step = 1000): #训练步数 train_step = 10 #初始化权重 weights=[random.random() for _ in range(X_train.shape[1]+1)] #训练集扩展为广义矩阵 X_train = np.hstack((X_train, np.ones(X_train.shape[0]).reshape(X_train.shape[0],1))) #定义梯度下降的step learning_rate = 0.001 #对于每个样本 sample_num = X_train.shape[0] for step in range(train_step): #预测 pred = self.sigmoid(np.dot(X_train,weights)) for i in range(sample_num): error = Y_train[i]- pred[i] weights = weights + learning_rate * error * X_train[i] if step % 1 == 0: acc = cal_accuracy([round(z) for z in pred], Y_train) print(&quot;训练过程第&quot;,step,&quot;步的准确率为：&quot;,acc) self.weights = weights def LR_test(self, X_test): X_test = np.hstack((X_test, np.ones(X_test.shape[0]).reshape(X_test.shape[0],1))) weights = self.weights pred = self.sigmoid(np.dot(X_test,weights)) return pred def cal_accuracy(pred, label): acc = list(pred-label).count(0)/len(pred-label) return acc if __name__==&quot;__main__&quot;: acc = 0 for i in range(1): X_train, X_test, Y_train, Y_test = load_data() lr = LR_classify() lr.LR_train(X_train, Y_train) pred = lr.LR_test(X_test) acc += cal_accuracy([round(z) for z in pred],Y_test) acc = acc/1 print(&quot;测试集准确率:&quot;,acc) 结果1234567891011训练过程第 0 步的准确率为： 0.46153846153846156训练过程第 1 步的准确率为： 0.46153846153846156训练过程第 2 步的准确率为： 0.46153846153846156训练过程第 3 步的准确率为： 0.6153846153846154训练过程第 4 步的准确率为： 0.8717948717948718训练过程第 5 步的准确率为： 0.8717948717948718训练过程第 6 步的准确率为： 0.9871794871794872训练过程第 7 步的准确率为： 0.9871794871794872训练过程第 8 步的准确率为： 1.0训练过程第 9 步的准确率为： 1.0测试集准确率: 1.0]]></content>
      <tags>
        <tag>tag1</tag>
        <tag>tag2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[朴素贝叶斯-NB]]></title>
    <url>%2F2018%2F11%2F06%2F%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%2F</url>
    <content type="text"><![CDATA[理论基础&emsp;&emsp;朴素贝叶斯是机器学习里的一种分类模型。本节将介绍朴素贝叶斯相关的理论基础知识。 相关知识&emsp;&emsp;贝叶斯学派的思想：$$先验概率+数据分布=后验概率$$ &emsp;&emsp;在这里举个例子帮助大家有个简单的理解：投掷一枚硬币100次，20次正面朝上，求这枚硬币正面朝上的概率$P(正)$。 频率学派：$P(正)=\frac{20}{100}=0.2$ 贝叶斯派： 先验概率：这枚硬币是从银行拿出来的，我认为它正面朝上的概率是0.5，并假设之前它已经被投掷1000次：500次正，500次负。 数据分布：本次的投掷结果：20次正，80次负。 后验概率：$P(正)=\frac{500+20}{1000+100}=0.473$ &emsp;&emsp;贝叶斯派被频率学派所诟病的就是所谓的先验概率。一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。 模型特点 朴素贝叶斯的模型能力：分类&emsp;给定样本x的特征$ X=\lbrace x_1,x_2,…,x_n\rbrace $,判断样本的标签$Y$。 朴素贝叶斯的学习方法：有监督学习&emsp;需要有训练样本对模型进行训练。 朴素贝叶斯的模型类型：生成式模型&emsp;通过训练样本计算特征$X$和标签$Y$的联合分布$P(X,Y)$。联合分布$P(X,Y)$可以看做是朴素贝叶斯的模型，里面的每个概率都是模型的参数。 朴素贝叶斯的特点：朴素&emsp;各维特征对标签的影响相互独立,$P(Y|X)=P(Y|x_1)P(Y|x_2)…P(Y|x_n)$ 数学基础&emsp;&emsp;先看条件独立公式，如果$X$和$Y$相互独立，$$P(X,Y)=P(X)P(Y)$$&emsp;&emsp;根据条件概率公式：$$P(X,Y)=P(X|Y)P(Y) = P(Y|X)P(X)$$&emsp;&emsp;可以推出贝叶斯公式：$$P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}$$&emsp;&emsp;其中，$P(Y)$表示类别标签的先验分布，$P(X|Y)$表示在已知类别标签$Y$的情况下，特征$X$取值的条件概率。$P(X)$表示特征的分布，所有类别下$P(X)$均是相同的。&emsp;&emsp;对于二分类问题，通过比较$P(Y=0|X)$和$P(Y=1|X)$,就可以实现分类。 模型推导&emsp;&emsp;假设我们有如下m个样本，每个样本有n维特征：$$\lbrace x_1^{(1)},x_2^{(1)},…,x_n^{(1)},y^{(1)}\rbrace,\lbrace x_1^{(2)},x_2^{(2)},…,x_n^{(2)},y^{(2)}\rbrace,…,\lbrace x_1^{(m)},x_2^{(m)},…,x_n^{(m)},y^{(m)}\rbrace$$&emsp;&emsp;训练过程：根据训练样本建立联合分布$P(X,Y)$。&emsp;&emsp;第一步：通过训练样本，可以计算训练样本中标签的分布$P(Y)$，通过最大似然估计，我们可以认为样本中类别标签的分布就是类别标签的先验分布$P(Y)$。最大似然估计：通俗来说就是已知样本分布的情况，得到能够导致这种情况(似然)最大概率发生的参数。假设所有标签为$Y=0$的训练样本个数共$m_0$个，所有标签为$Y=1$的训练样本个数共$m_1$个。则$P(Y=0)=\frac{m_0}{m_0+m_1}$。&emsp;&emsp;第二步：针对每种标签下的样本，统计每一维特征$x$的分布，得到条件分布$P(x|Y)$。例如针对所有标签为$Y=0$的训练样本，统计第$i$维特征$x_i$的取值，假设有两种取值0和1，取值为0共$i_0$个,则$i_1=m_0-i_0$。则条件概率$P(x_i=0|Y=0)=\frac{i_0}{m_0}$。&emsp;&emsp;第三步：根据标签的先验分布$P(Y)$和每维特征基于标签的条件分布$P(x|Y)$，建立联合分布$P(X,Y)$。例如$P(x_i=0,Y=0)=P(x_i=0|Y=0)P(Y=0)=\frac{i_0}{m_0}\times\frac{m_0}{m_0+m_1}=\frac{i_0}{m_0+m_1}$。注：这里联合分布$P(X,Y)$就可以看成是模型训练的参数。&emsp;&emsp;预测过程：根据联合分布$P(X,Y)$和特征$X$,判断$P(Y=0|X)$与$P(Y=1|X)$的大小。&emsp;&emsp;第一步：根据预测样本的每维特征$x_i$和联合分布$P(X,Y)$，都可以计算$P(Y=0|{x_i})$和$P(Y=1|{x_i})$。&emsp;&emsp;第二步：根据特征对标签的影响相互独立，我们有：$$P(Y|X)=P(Y|{x_1}{x_2}…{x_n})=P(Y|{x_1})P(Y|{x_2})…P(Y|{x_n})$$&emsp;&emsp;第三步：比较$P(Y=1|X)$和$P(Y=0|X)$，确定标签。 实例讲解&emsp;&emsp;接下来，我会有一个简单的例子，帮助大家充分理解上述讲的过程。&emsp;&emsp;假设我们拿到了10个男生的信息，并让一位女生根据这些信息进行判断是否考虑嫁给他们（下述随机举例，不代表真实情况）。 特征 高 富 帅 温柔 女生嫁不嫁 男生1 高 富 帅 温柔 嫁 男生2 高 穷 搓 温柔 不嫁 男生3 矮 穷 帅 不温柔 不嫁 男生4 矮 穷 搓 温柔 不嫁 男生5 矮 富 帅 不温柔 不嫁 男生6 高 穷 帅 温柔 嫁 男生7 矮 穷 帅 温柔 不嫁 男生8 矮 富 搓 温柔 嫁 男生9 高 穷 搓 不温柔 不嫁 男生10 高 富 帅 不温柔 嫁 现在，有一个男生x（样本），他高、富、搓、温柔（特征），请问这个女生是否会选择嫁给他（分类）。这就是个典型的分类问题。根据贝叶斯公式：$$P（嫁|高、富、搓、温柔） = \frac{P(嫁)P(高、富、搓、温柔|嫁)}{P(高、富、搓、温柔)}$$ 第一步：我们需要考虑女生嫁人的主观意愿，也就是标签的先验分布$P(嫁)$。&emsp;&emsp;就是无论对方男生条件怎么样，这个女生自己到底有多想嫁人。根据上述的10个样本，我们可以知道，这个女生想嫁人的概率$P(嫁)=\frac{4}{10}$，这个女生不想嫁人的概率$P(不嫁)=\frac{6}{10}$。 这里有一点需要说明:先验分布我们是不知道的，也不是能算的(女生的心思你不懂)。但是现实的结果是10个男生里，女生想嫁4个人。所以我们可以假定在某种先验分布的情况下，让这种现实结果出现的概率最大。所以就得到了先验分布$P(嫁)=\frac{4}{10}$。&emsp;&emsp;我们想象一个极端的例子，如果10个男生里，女生的选择全是不嫁，我们可以推断出这个女生可能在当前阶段是个独身主义者，完全不考虑嫁给任何人（即使样本不能代表任何人,比如王校长），即$P(嫁)=0$。即使样本是王校长,朴素贝叶斯模型也会认为女生选择不嫁。 第二步：我们知道了女生嫁人的主观意愿，我们还需要知道，如果女生选择嫁的给男生x，他的高、富、搓对嫁这个结果分别有多大的影响，也就是条件概率$P(高、富、搓、温柔|嫁)$。&emsp;&emsp;接下来就要用到朴素贝叶斯的“朴素”。朴素贝叶斯认为，男生的高、富、搓（特征）对嫁（标签）这个结果的影响是相互独立的，也就是$P(高、富、搓、温柔|嫁)=P(高|嫁)P(富|嫁)P(搓|嫁)P(温柔|嫁)$。 这里有一点需要说明:为什么要假设特征对标签的影响是相互独立的呢？因为如果不独立，特征空间就是1个三维空间，特征总共2*2*2=8，如果独立呢，特征空间就是3个一维空间，2+2+2=6。一个累乘，一个累加。在现实生活中，往往有非常多的特征，每维特征的取值也是非常之多，而“朴素”正是以牺牲准确率为代价来大幅降低计算强度。 我们选择重新整理下样本，只看选择嫁的这4个男生 特征 高 富 帅 温柔 女生嫁不嫁 男生1 高 富 帅 温柔 嫁 男生6 高 穷 帅 温柔 嫁 男生8 矮 富 搓 温柔 嫁 男生10 高 富 帅 不温柔 嫁 $P(高|嫁)$的计算方式：在所有选择嫁的男生里，高的男生占几个。可以得到$P(高|嫁)=\frac{3}{4}$。同理可以得到$P(富|嫁)=\frac{3}{4}$，$P(搓|嫁)=\frac{1}{4}$，$P(温柔|嫁)=\frac{3}{4}$。 对于同一个样本而言，$P(X)$都是一样的，故只需要比较贝叶斯公式的分子$P(Y=0)P(X|Y=0)$和$P(Y=1)P(X|Y=1)$即可。我们可以计算得到$$P(嫁)P(高、富、搓、温柔|嫁)=\frac{4}{10}\times\frac{3}{4}\times\frac{3}{4}\times\frac{1}{4}\times\frac{3}{4}=\frac{9}{640}$$ $$P(不嫁)P(高、富、搓、温柔|不嫁)=\frac{6}{10}\times\frac{2}{6}\times\frac{1}{6}\times\frac{1}{6}\times\frac{3}{6}=\frac{1}{120}=\frac{9}{1080}$$ 可以得到：$P(嫁|高、富、搓)&gt;P(不嫁|高、富、搓)$朴素贝叶斯模型就可以告诉这个女生，对于男生x：嫁 ！ 进阶优化由于朴素贝叶斯模型里假设特征相互独立，当计算联合概率时，就可能用到各维特征空间中概率的累乘。累乘存在一个致命的缺点：一旦其中一项为0，整项就为0。当特征很多时，非常容易出现稀疏情况，$P(x_i|Y)=0$，导致$P(X={x_1,x_2,…,x_i,…,x_n}|Y)=0$，直接导致最后的结果$P(Y|X)=0$。显然不够合理。为了避免这种情况发生，可以采取平滑策略。最常见的平滑策略就是拉普拉斯平滑，即在统计计算概率时，默认某类样本中，在各维空间中各个取值至少出现了一次。如果训练样本集数量充分大时，并不会对结果产生影响，并且解决了上述概率为0的尴尬局面。 优缺点优点 思路简单 空间复杂度低（联合概率$P(X,Y)$只需要二维存储） 时间复杂度低（$P(X),P(Y),P(X|Y)$可以从联合分布$P(X,Y)$中直接计算） 缺点 假设了各维对结果的影响相互独立，现实中不成立。（比如富就会对帅有影响，有钱更容易打扮变帅。）当特征之间影响较大，朴素贝叶斯的性能就不太好。 工程实现代码中NB_classify为朴素贝叶斯模型，具体代码如下。 代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115# -*- coding: utf-8 -*-&quot;&quot;&quot;Created on Thu Aug 23 19:52:02 2018@author: huangzhaolong&quot;&quot;&quot;#利用sklearn自带的鸢尾花数据集from sklearn.datasets import load_irisimport randomdef load_data(): #加载鸢尾花数据集 iris = load_iris() X = iris.data Y = iris.target #特征离散化 X = X.round() data_num = X.shape[0] index_list = [random.random() &gt;= 0.2 for _ in range(data_num)] train_index_list = [] test_index_list = [] for i in range(len(index_list)): if index_list[i]: train_index_list.append(i) else: test_index_list.append(i) X_train = X[train_index_list] X_test = X[test_index_list] Y_train = Y[train_index_list] Y_test = Y[test_index_list] return X_train, X_test, Y_train, Y_testclass NB_classify(object): def NB_train(self, X_train, Y_train): &apos;&apos;&apos; 在已知X和Y的情况下，构建联合分布P(X,Y),即参数计算，计算方式为统计 根据贝叶斯公式，计算P(X,Y)=P(X|Y)*P(Y) &apos;&apos;&apos; p_xy = [] #对于每个类别 p_y = &#123;&#125; for k in set(Y_train): total_xi_y = list(Y_train).count(k) print(&quot;对于类别为&quot;,k,&quot;的样本共有&quot;,total_xi_y,&quot;个&quot;) #计算P(Y) p_y.setdefault(k,list(Y_train).count(k)/Y_train.shape[0]) p_x_y_sub = [] #对每个维度的特征，计算P(x|Y) for i in range(X_train.shape[1]): #对于每一维特征,统计当前类别下样本个数,得到P(x|Y) p_x_sub_y_sub = &#123;&#125; print(&quot;对于类别为&quot;,k,&quot;的样本，第&quot;,i,&quot;维的离散特征共有&quot;,len(set([(round(item)) for item in X_train[[l for l in range(len(Y_train)) if Y_train[l] == k],i]])),&quot;种&quot;) #对每个维度下特征的取值，计算p(x=xi|Y) for j in set(X_train[[l for l in range(len(Y_train)) if Y_train[l] == k],i]): #统计每个类别下，每个特征占该类别样本个数的比例,并做贝叶斯平滑 print(&quot;对于类别为&quot;,k,&quot;的样本，第&quot;,i,&quot;维特征为&quot;,j,&quot;的样本共有&quot;,list(X_train[[l for l in range(len(Y_train)) if Y_train[l] == k],i]).count(j), &quot;个&quot;) #计算p(y)*p(x|y) p_x_sub_y_sub.setdefault(j,p_y[k]*(list(X_train[[l for l in range(len(Y_train)) if Y_train[l] == k],i]).count(j)+1)/(total_xi_y+len(set(X_train[[l for l in range(len(Y_train)) if Y_train[l] == k],i])))) p_x_y_sub.append(p_x_sub_y_sub) &apos;&apos;&apos; 最后得到P(X|Y)的模型参数，即矩阵M(xy)，其中矩阵M为m行n列,m为类别个数，n为特征维数。每个元素是一个字典，字典的key是该维特征的取值，字典的value是概率 例如矩阵M中的元素M_ij，M_ij是一个字典。 M_ij的key是第j维特征的所有取值，例如k_j M_ij的value是第j维特征取值为k_j时，类别为i的概率P(xj=k_j|i)，也就是所有类别为i的样本下，第j维特征为k_j的概率v_ij。 v_ij = 所有类别为i的样本下，第j维特征为k_j的样本数数/所有类别为i的样本数 &apos;&apos;&apos; p_xy.append(p_x_y_sub) print(&quot;P(Y)的先验概率：\n&quot;,p_y) print(&quot;P(X,Y)的联合概率：\n&quot;,p_xy) self.p_xy = p_xy def NB_test(self, X_test): p_xy = self.p_xy pred = [] #对于每个样本 for z in range(len(X_test)): #对于每个类别 pj = [1]*3 for j in range(3): #对于每个特征,通过联合概率分布，计算每个特征下得到的类别P(yj|xi) for i in range(len(X_test[z])): if X_test[z][i] in p_xy[j][i]: p_xi_yj = p_xy[j][i][X_test[z][i]] else: #如果训练集没有出现过该特征，则说明构建的P(X,Y)联合分布并不准确，生成式模型需要训练样本足够大，足够全 p_xi_yj = 0.0000001 #计算全部特征到每个类别的概率P(yj|X)，这里是认为各特征对类别是独立的，可以连乘 pj[j] = pj[j]*p_xi_yj #根据概率最大计算最可能的类别，即 argmax P(yj|X) pred.append(pj.index(max(pj))) return pred def cal_accuracy(pred, Y_test): acc = list(pred-Y_test).count(0)/len(pred-Y_test) return acc if __name__==&quot;__main__&quot;: acc = 0 for i in range(1): X_train, X_test, Y_train, Y_test = load_data() nb = NB_classify() nb.NB_train(X_train, Y_train) pred = nb.NB_test(X_test) acc += cal_accuracy(pred,Y_test) acc = acc/1 print(&quot;acc:&quot;,acc) 结果12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152对于类别为 0 的样本共有 42 个对于类别为 0 的样本，第 0 维的离散特征共有 3 种对于类别为 0 的样本，第 0 维特征为 4.0 的样本共有 4 个对于类别为 0 的样本，第 0 维特征为 5.0 的样本共有 34 个对于类别为 0 的样本，第 0 维特征为 6.0 的样本共有 4 个对于类别为 0 的样本，第 1 维的离散特征共有 2 种对于类别为 0 的样本，第 1 维特征为 3.0 的样本共有 25 个对于类别为 0 的样本，第 1 维特征为 4.0 的样本共有 17 个对于类别为 0 的样本，第 2 维的离散特征共有 2 种对于类别为 0 的样本，第 2 维特征为 1.0 的样本共有 17 个对于类别为 0 的样本，第 2 维特征为 2.0 的样本共有 25 个对于类别为 0 的样本，第 3 维的离散特征共有 2 种对于类别为 0 的样本，第 3 维特征为 0.0 的样本共有 41 个对于类别为 0 的样本，第 3 维特征为 1.0 的样本共有 1 个对于类别为 1 的样本共有 40 个对于类别为 1 的样本，第 0 维的离散特征共有 3 种对于类别为 1 的样本，第 0 维特征为 5.0 的样本共有 5 个对于类别为 1 的样本，第 0 维特征为 6.0 的样本共有 27 个对于类别为 1 的样本，第 0 维特征为 7.0 的样本共有 8 个对于类别为 1 的样本，第 1 维的离散特征共有 2 种对于类别为 1 的样本，第 1 维特征为 2.0 的样本共有 12 个对于类别为 1 的样本，第 1 维特征为 3.0 的样本共有 28 个对于类别为 1 的样本，第 2 维的离散特征共有 3 种对于类别为 1 的样本，第 2 维特征为 3.0 的样本共有 3 个对于类别为 1 的样本，第 2 维特征为 4.0 的样本共有 24 个对于类别为 1 的样本，第 2 维特征为 5.0 的样本共有 13 个对于类别为 1 的样本，第 3 维的离散特征共有 2 种对于类别为 1 的样本，第 3 维特征为 1.0 的样本共有 29 个对于类别为 1 的样本，第 3 维特征为 2.0 的样本共有 11 个对于类别为 2 的样本共有 41 个对于类别为 2 的样本，第 0 维的离散特征共有 4 种对于类别为 2 的样本，第 0 维特征为 8.0 的样本共有 5 个对于类别为 2 的样本，第 0 维特征为 5.0 的样本共有 1 个对于类别为 2 的样本，第 0 维特征为 6.0 的样本共有 23 个对于类别为 2 的样本，第 0 维特征为 7.0 的样本共有 12 个对于类别为 2 的样本，第 1 维的离散特征共有 3 种对于类别为 2 的样本，第 1 维特征为 2.0 的样本共有 4 个对于类别为 2 的样本，第 1 维特征为 3.0 的样本共有 34 个对于类别为 2 的样本，第 1 维特征为 4.0 的样本共有 3 个对于类别为 2 的样本，第 2 维的离散特征共有 4 种对于类别为 2 的样本，第 2 维特征为 4.0 的样本共有 1 个对于类别为 2 的样本，第 2 维特征为 5.0 的样本共有 17 个对于类别为 2 的样本，第 2 维特征为 6.0 的样本共有 20 个对于类别为 2 的样本，第 2 维特征为 7.0 的样本共有 3 个对于类别为 2 的样本，第 3 维的离散特征共有 2 种对于类别为 2 的样本，第 3 维特征为 1.0 的样本共有 1 个对于类别为 2 的样本，第 3 维特征为 2.0 的样本共有 40 个P(Y)的先验概率： &#123;0: 0.34146341463414637, 1: 0.3252032520325203, 2: 0.3333333333333333&#125;P(X,Y)的联合概率： [[&#123;4.0: 0.03794037940379404, 5.0: 0.2655826558265583, 6.0: 0.03794037940379404&#125;, &#123;3.0: 0.2017738359201774, 4.0: 0.13968957871396895&#125;, &#123;1.0: 0.13968957871396895, 2.0: 0.2017738359201774&#125;, &#123;0.0: 0.3259423503325942, 1.0: 0.015521064301552107&#125;], [&#123;5.0: 0.04537719795802609, 6.0: 0.21176025713745508, 7.0: 0.06806579693703914&#125;, &#123;2.0: 0.10065814943863724, 3.0: 0.22454510259388305&#125;, &#123;3.0: 0.030251465305350726, 4.0: 0.189071658158442, 5.0: 0.10588012856872754&#125;, &#123;1.0: 0.23228803716608595, 2.0: 0.09291521486643438&#125;], [&#123;8.0: 0.044444444444444446, 5.0: 0.014814814814814814, 6.0: 0.17777777777777778, 7.0: 0.0962962962962963&#125;, &#123;2.0: 0.03787878787878787, 3.0: 0.26515151515151514, 4.0: 0.0303030303030303&#125;, &#123;4.0: 0.014814814814814814, 5.0: 0.13333333333333333, 6.0: 0.15555555555555556, 7.0: 0.029629629629629627&#125;, &#123;1.0: 0.015503875968992248, 2.0: 0.3178294573643411&#125;]]acc: 0.9629629629629629]]></content>
      <tags>
        <tag>tag1</tag>
        <tag>tag2</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F11%2F06%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
